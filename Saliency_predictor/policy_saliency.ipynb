{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch, os, gym, time, glob, argparse, sys\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "from scipy.misc import imresize # preserves single-pixel info _unlike_ img = img[::2,::2]\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import collections\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Saliency Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GazeNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GazeNetwork, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(4, 32, 4, stride=2, bias = True)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, stride=2, bias = True)\n",
    "    self.conv3 = nn.Conv2d(64, 64, 5, stride=2, bias = True)\n",
    "    self.deconv3 = nn.ConvTranspose2d(64,64,5,stride = 2, bias = True)\n",
    "    self.deconv2 = nn.ConvTranspose2d(64,32,3,stride = 2, bias = True)\n",
    "    self.deconv1 = nn.ConvTranspose2d(32,1,4,stride = 2, bias = True)\n",
    "    self.logsoftmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "  def forward(self, X, train=True, hard=False):\n",
    "    X = F.elu(self.conv1(X))\n",
    "    X = F.elu(self.conv2(X))\n",
    "    X = F.elu(self.conv3(X))\n",
    "    X = F.elu(self.deconv3(X))\n",
    "    X = F.elu(self.deconv2(X))\n",
    "    X = self.deconv1(X)\n",
    "    \n",
    "    return self.logsoftmax(X.view(-1,1,6400)).view(-1,1,80,80)\n",
    "  \n",
    "  def load_model(self, path):\n",
    "    self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gazenet = GazeNetwork()#.train(False)\n",
    "gazenet.load_model('model/saliency_model_5.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Make  the policy trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--env', default='Pong-v4', type=str, help='gym environment')\n",
    "    parser.add_argument('--processes', default=20, type=int, help='number of processes to train with')\n",
    "    parser.add_argument('--render', default=False, type=bool, help='renders the atari environment')\n",
    "    parser.add_argument('--test', default=False, type=bool, help='sets lr=0, chooses most likely actions')\n",
    "    parser.add_argument('--rnn_steps', default=50, type=int, help='steps to train LSTM over')\n",
    "    parser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
    "    parser.add_argument('--seed', default=1, type=int, help='seed random # generators (for reproducibility)')\n",
    "    parser.add_argument('--gamma', default=0.99, type=float, help='rewards discount factor')\n",
    "    parser.add_argument('--tau', default=1.0, type=float, help='generalized advantage estimation discount')\n",
    "    parser.add_argument('--horizon', default=0.8, type=float, help='horizon for running averages')\n",
    "    parser.add_argument('--hidden', default=32, type=int, help='hidden size of GRU')\n",
    "    return parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount = lambda x, gamma: lfilter([1],[1,-gamma],x[::-1])[::-1] # discounted rewards one liner\n",
    "prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32)/255.\n",
    "\n",
    "def get_saliency(states, net= gazenet):\n",
    "  sal = net(torch.FloatTensor(states).view(1,4,80,80))\n",
    "  sal = sal.data.numpy()\n",
    "  sal = np.exp(sal)\n",
    "  sal = sal/sal.max()\n",
    "  sal = np.minimum(sal,.1)\n",
    "  return torch.tensor(np.multiply(sal, states[-1]))\n",
    "  \n",
    "def printlog(args, s, end='\\n', mode='a'):\n",
    "    print(s, end=end) ; f=open(args.save_dir+'log.txt',mode) ; f.write(s+'\\n') ; f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v4')\n",
    "state = env.reset()\n",
    "frames = []\n",
    "seq = collections.deque(maxlen = 4)\n",
    "for _ in range(4):\n",
    "  seq.append(np.zeros((80,80)))\n",
    "\n",
    "state = prepro(state)\n",
    "seq.append(state)\n",
    "for i in range(50):\n",
    "  action = np.random.randint(6)\n",
    "  frame, reward, done, _ = env.step(action)\n",
    "  \n",
    "  state = prepro(frame)\n",
    "  seq.append(state)\n",
    "  state_sal = get_saliency(seq)\n",
    "  frames.append(state_sal)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80, 80])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJsAAAJCCAYAAADXz7VCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE/l2wlEPyuoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109efa208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "fig, ax = plt.subplots(nrows = 4, figsize = (20,10))\n",
    "\n",
    "print(frames[10].data.numpy()[0,0,10,:])\n",
    "for i in range(1,5):\n",
    "  ax[i-1].imshow(frames[i*10].data.numpy()[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0644116401672363"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = []\n",
    "for _ in range(50):\n",
    "  start = time.time()\n",
    "\n",
    "  k = get_saliency(seq)\n",
    "\n",
    "  times.append(time.time()-start)\n",
    "26000*np.mean(times)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, num_actions, channels, memsize):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 4, 3, stride=2, padding=1, bias = None)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, stride=2, padding=1, bias = None)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 5, stride=4, padding=2, bias = None)\n",
    "        self.conv4 = nn.Conv2d(16, 32, 3, stride=2, padding=1, bias = None)\n",
    "        self.conv5 = nn.Conv2d(32, 64, 3, stride=1, padding=0, bias = None)\n",
    "        self.mem = nn.LSTMCell(64, memsize, bias = None)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1, bias = None), nn.Linear(memsize, num_actions, bias = None)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inp, hx, cx = inputs\n",
    "        x = F.elu(self.conv1(inp))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = F.elu(self.conv5(x))\n",
    "        hx, cx = self.mem(x.view(-1, 64 * 1 * 1), (hx, cx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx, cx\n",
    "\n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; step = 0\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [int(s.split('.')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; step = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if step is 0 else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 3]) 36\n",
      "torch.Size([8, 4, 3, 3]) 288\n",
      "torch.Size([16, 8, 5, 5]) 3200\n",
      "torch.Size([32, 16, 3, 3]) 4608\n",
      "torch.Size([64, 32, 3, 3]) 18432\n",
      "torch.Size([128, 64]) 8192\n",
      "torch.Size([128, 32]) 4096\n",
      "torch.Size([1, 32]) 32\n",
      "torch.Size([6, 32]) 192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39076"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NNPolicy(6,1,32)\n",
    "count = 0\n",
    "for f in model.parameters():\n",
    "  count += np.prod(f.size())\n",
    "  print(f.size(), np.prod(f.size()))\n",
    "  \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4608"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*16*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_func(args, values, logps, actions, rewards):\n",
    "    np_values = values.view(-1).data.numpy()\n",
    "\n",
    "    # generalized advantage estimation using \\delta_t residuals (a policy gradient method)\n",
    "    delta_t = np.asarray(rewards) + args.gamma * np_values[1:] - np_values[:-1]\n",
    "    logpys = logps.gather(1, torch.tensor(actions).view(-1,1))\n",
    "    gen_adv_est = discount(delta_t, args.gamma * args.tau)\n",
    "    policy_loss = -(logpys.view(-1) * torch.FloatTensor(gen_adv_est.copy())).sum()\n",
    "    \n",
    "    # l2 loss over value estimator\n",
    "    rewards[-1] += args.gamma * np_values[-1]\n",
    "    discounted_r = discount(np.asarray(rewards), args.gamma)\n",
    "    discounted_r = torch.tensor(discounted_r.copy(), dtype=torch.float32)\n",
    "    value_loss = .5 * (discounted_r - values[:-1,0]).pow(2).sum()\n",
    "    \n",
    "    entropy_loss = -(-logps * torch.exp(logps)).sum() # encourage lower entropy\n",
    "    \n",
    "    return policy_loss + 0.5 * value_loss# + 0.01 * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, args, info, rank = 0):\n",
    "    env = gym.make(args.env) # make a local (unshared) environment\n",
    "    \n",
    "    # seed everything\n",
    "    #env.seed(args.seed + rank)\n",
    "    #torch.manual_seed(args.seed + rank) \n",
    "    \n",
    "    #model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions) # a local/unshared model\n",
    "    \n",
    "    state_seq = collections.deque(maxlen = 4)\n",
    "    for _ in range(4):\n",
    "      state_seq.append(np.zeros((80,80)))\n",
    "    \n",
    "    state = prepro(env.reset()) # get first state\n",
    "    state_seq.append(state)\n",
    "    state_sal = get_saliency(state_seq)\n",
    "\n",
    "    start_time = last_disp_time = time.time()\n",
    "    episode_length, epr, eploss, done  = 0, 0, 0, True # bookkeeping\n",
    "\n",
    "    while info['frames'][0] <= 8e7 or args.test: # openai baselines uses 40M frames...we'll use 80M\n",
    "        #model.load_state_dict(shared_model.state_dict()) # sync with shared model\n",
    "        \n",
    "        hx = torch.zeros(1, args.hidden) if done else hx.detach()  # rnn activation vector\n",
    "        cx = torch.zeros(1, args.hidden) if done else cx.detach()\n",
    "          \n",
    "        values, logps, actions, rewards = [], [], [], [] # save values for computing gradientss\n",
    "\n",
    "        for step in range(args.rnn_steps):\n",
    "            episode_length += 1\n",
    "            \n",
    "            value, logit, hx, cx = model((state_sal.view(1,1,80,80), hx,cx))\n",
    "            logp = F.log_softmax(logit, dim=-1)                             \n",
    "            \n",
    "            action = torch.exp(logp).multinomial(num_samples=1).data[0]#logp.max(1)[1].data if args.test else\n",
    "                                         \n",
    "            state, reward, done, _ = env.step(action.numpy()[0])\n",
    "            \n",
    "            if args.render: env.render()\n",
    "\n",
    "              \n",
    "            start = time.time()\n",
    "            state_seq.append(prepro(state))\n",
    "            state_sal = get_saliency(state_seq)\n",
    "            \n",
    "            epr += reward\n",
    "                                         \n",
    "            reward = np.clip(reward, -1, 1) # reward\n",
    "            done = done or episode_length >= 1e4 # don't playing one ep for too long\n",
    "            \n",
    "            info['frames'].add_(1)\n",
    "            num_frames = int(info['frames'].item())\n",
    "                                         \n",
    "            if num_frames % 1e6 == 0: # save every 1M frames\n",
    "                printlog(args, '\\n\\t{:.0f}M frames: saved model\\n'.format(num_frames/1e6))\n",
    "                torch.save(model.state_dict(), args.save_dir+'model.{:.0f}.tar'.format(num_frames/1e6))\n",
    "\n",
    "            if done: # update shared data\n",
    "                info['episodes'] += 1\n",
    "                interp = 1 if info['episodes'][0] == 1 else 1 - args.horizon\n",
    "                info['run_epr'].mul_(1-interp).add_(interp * epr)\n",
    "\n",
    "            if time.time() - last_disp_time > 300: # print info ~ every 5 minute\n",
    "                elapsed = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start_time))\n",
    "                printlog(args, 'time {}, episodes {:.0f}, frames {:.3f}M, mean epr {:.2f}'\n",
    "                    .format(elapsed, info['episodes'].item(), num_frames/1e6,\n",
    "                    info['run_epr'].item()))\n",
    "                last_disp_time = time.time()\n",
    "\n",
    "            if done: # maybe print info.\n",
    "                episode_length, epr = 0, 0\n",
    "                state = torch.tensor(prepro(env.reset()))\n",
    "\n",
    "            values.append(value) ; logps.append(logp) ; actions.append(action) ; rewards.append(reward)\n",
    "        \n",
    "        # add a final state value function for calculating the losses.\n",
    "        next_value = torch.zeros(1,1) if done else model((state_sal.view(1,1,80,80), hx,cx))[0]\n",
    "        values.append(next_value.detach())\n",
    "\n",
    "        loss = cost_func(args, torch.cat(values), torch.cat(logps), torch.cat(actions), np.asarray(rewards))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "        #\n",
    "        #for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "        #    if shared_param.grad is None: shared_param._grad = param.grad # sync gradients with shared model\n",
    "        #shared_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: pong-v4/model.3.tar\n",
      "Model information:\n",
      "torch.Size([4, 1, 3, 3]) 36\n",
      "torch.Size([8, 4, 3, 3]) 288\n",
      "torch.Size([16, 8, 5, 5]) 3200\n",
      "torch.Size([32, 16, 3, 3]) 4608\n",
      "torch.Size([64, 32, 3, 3]) 18432\n",
      "torch.Size([128, 64]) 8192\n",
      "torch.Size([128, 32]) 4096\n",
      "torch.Size([1, 32]) 32\n",
      "torch.Size([6, 32]) 192\n",
      "39076\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 00h 05m 00s, episodes 19, frames 0.028M, mean epr -19.00\n",
      "time 00h 10m 00s, episodes 40, frames 0.058M, mean epr -19.47\n",
      "time 00h 15m 00s, episodes 62, frames 0.089M, mean epr -19.66\n",
      "time 00h 20m 00s, episodes 82, frames 0.120M, mean epr -18.96\n",
      "time 00h 25m 00s, episodes 103, frames 0.150M, mean epr -19.88\n",
      "time 00h 30m 00s, episodes 124, frames 0.181M, mean epr -18.77\n",
      "time 00h 35m 00s, episodes 144, frames 0.210M, mean epr -19.14\n",
      "time 00h 40m 00s, episodes 165, frames 0.241M, mean epr -19.20\n",
      "time 00h 45m 00s, episodes 187, frames 0.270M, mean epr -19.73\n",
      "time 00h 50m 00s, episodes 209, frames 0.301M, mean epr -19.59\n",
      "time 00h 55m 00s, episodes 229, frames 0.331M, mean epr -17.77\n",
      "time 01h 00m 00s, episodes 249, frames 0.359M, mean epr -19.48\n",
      "time 01h 05m 00s, episodes 269, frames 0.388M, mean epr -18.50\n",
      "time 01h 10m 00s, episodes 288, frames 0.416M, mean epr -19.16\n",
      "time 01h 15m 00s, episodes 306, frames 0.443M, mean epr -19.60\n",
      "time 01h 20m 00s, episodes 327, frames 0.472M, mean epr -19.43\n",
      "time 01h 25m 00s, episodes 347, frames 0.502M, mean epr -19.69\n",
      "time 01h 30m 00s, episodes 367, frames 0.531M, mean epr -19.60\n",
      "time 01h 35m 00s, episodes 387, frames 0.559M, mean epr -19.37\n",
      "time 01h 40m 00s, episodes 407, frames 0.589M, mean epr -18.97\n",
      "time 01h 45m 00s, episodes 427, frames 0.619M, mean epr -18.75\n",
      "time 01h 50m 00s, episodes 447, frames 0.648M, mean epr -19.45\n",
      "time 01h 55m 00s, episodes 466, frames 0.677M, mean epr -19.15\n",
      "time 02h 00m 00s, episodes 488, frames 0.708M, mean epr -19.77\n",
      "time 02h 05m 00s, episodes 507, frames 0.738M, mean epr -18.80\n",
      "time 02h 10m 00s, episodes 526, frames 0.767M, mean epr -18.83\n",
      "time 02h 15m 00s, episodes 547, frames 0.798M, mean epr -19.14\n",
      "time 02h 20m 00s, episodes 568, frames 0.827M, mean epr -19.48\n",
      "time 02h 25m 00s, episodes 585, frames 0.854M, mean epr -18.03\n",
      "time 02h 30m 00s, episodes 605, frames 0.883M, mean epr -19.57\n",
      "time 02h 35m 00s, episodes 624, frames 0.912M, mean epr -18.98\n",
      "time 02h 40m 00s, episodes 644, frames 0.942M, mean epr -18.51\n",
      "time 02h 45m 00s, episodes 663, frames 0.972M, mean epr -18.81\n",
      "\n",
      "\t1M frames: saved model\n",
      "\n",
      "time 02h 50m 00s, episodes 682, frames 1.001M, mean epr -18.96\n",
      "time 02h 55m 00s, episodes 702, frames 1.029M, mean epr -18.93\n",
      "time 03h 00m 00s, episodes 722, frames 1.059M, mean epr -19.19\n",
      "time 03h 05m 00s, episodes 739, frames 1.084M, mean epr -19.61\n",
      "time 03h 10m 00s, episodes 758, frames 1.113M, mean epr -18.42\n",
      "time 03h 15m 00s, episodes 777, frames 1.142M, mean epr -19.46\n",
      "time 03h 20m 00s, episodes 796, frames 1.171M, mean epr -18.68\n",
      "time 03h 25m 00s, episodes 815, frames 1.199M, mean epr -18.95\n",
      "time 03h 30m 00s, episodes 835, frames 1.228M, mean epr -18.48\n",
      "time 03h 35m 00s, episodes 854, frames 1.259M, mean epr -17.92\n",
      "time 03h 40m 00s, episodes 874, frames 1.289M, mean epr -19.07\n",
      "time 03h 45m 00s, episodes 893, frames 1.319M, mean epr -18.14\n",
      "time 03h 50m 00s, episodes 913, frames 1.349M, mean epr -18.92\n",
      "time 03h 55m 00s, episodes 933, frames 1.380M, mean epr -18.95\n",
      "time 04h 00m 00s, episodes 953, frames 1.410M, mean epr -18.76\n",
      "time 04h 05m 00s, episodes 972, frames 1.440M, mean epr -18.49\n",
      "time 04h 10m 00s, episodes 990, frames 1.467M, mean epr -19.41\n",
      "time 04h 15m 00s, episodes 1009, frames 1.495M, mean epr -18.82\n",
      "time 04h 20m 00s, episodes 1027, frames 1.524M, mean epr -19.04\n",
      "time 04h 25m 00s, episodes 1046, frames 1.553M, mean epr -18.65\n",
      "time 04h 30m 00s, episodes 1064, frames 1.583M, mean epr -18.01\n",
      "time 04h 35m 00s, episodes 1082, frames 1.612M, mean epr -18.19\n",
      "time 04h 40m 00s, episodes 1099, frames 1.641M, mean epr -18.09\n",
      "time 04h 45m 00s, episodes 1118, frames 1.672M, mean epr -18.78\n",
      "time 04h 50m 00s, episodes 1137, frames 1.702M, mean epr -19.15\n",
      "time 04h 55m 00s, episodes 1157, frames 1.732M, mean epr -18.94\n",
      "time 05h 00m 00s, episodes 1177, frames 1.762M, mean epr -20.04\n",
      "time 05h 05m 00s, episodes 1196, frames 1.792M, mean epr -18.59\n",
      "time 05h 10m 00s, episodes 1214, frames 1.822M, mean epr -18.60\n",
      "time 05h 15m 00s, episodes 1233, frames 1.853M, mean epr -18.72\n",
      "time 05h 20m 00s, episodes 1252, frames 1.883M, mean epr -18.70\n",
      "time 05h 25m 00s, episodes 1271, frames 1.913M, mean epr -19.09\n",
      "time 05h 30m 00s, episodes 1289, frames 1.944M, mean epr -18.83\n",
      "time 05h 35m 00s, episodes 1307, frames 1.974M, mean epr -18.19\n",
      "\n",
      "\t2M frames: saved model\n",
      "\n",
      "time 05h 40m 00s, episodes 1327, frames 2.004M, mean epr -19.06\n",
      "time 05h 45m 00s, episodes 1345, frames 2.035M, mean epr -17.79\n",
      "time 05h 50m 00s, episodes 1365, frames 2.065M, mean epr -19.22\n",
      "time 05h 55m 00s, episodes 1383, frames 2.095M, mean epr -17.19\n",
      "time 06h 00m 00s, episodes 1402, frames 2.126M, mean epr -18.57\n",
      "time 06h 05m 00s, episodes 1421, frames 2.156M, mean epr -18.78\n",
      "time 06h 10m 00s, episodes 1440, frames 2.186M, mean epr -18.91\n",
      "time 06h 15m 00s, episodes 1457, frames 2.216M, mean epr -18.34\n",
      "time 06h 20m 00s, episodes 1475, frames 2.246M, mean epr -18.32\n",
      "time 06h 25m 00s, episodes 1493, frames 2.276M, mean epr -18.64\n",
      "time 06h 30m 00s, episodes 1511, frames 2.306M, mean epr -17.17\n",
      "time 06h 35m 00s, episodes 1527, frames 2.335M, mean epr -18.29\n",
      "time 06h 40m 00s, episodes 1545, frames 2.364M, mean epr -18.41\n",
      "time 06h 45m 00s, episodes 1562, frames 2.393M, mean epr -17.49\n",
      "time 06h 50m 00s, episodes 1578, frames 2.422M, mean epr -18.19\n",
      "time 06h 55m 00s, episodes 1595, frames 2.451M, mean epr -17.86\n",
      "time 07h 00m 00s, episodes 1611, frames 2.479M, mean epr -17.75\n",
      "time 07h 05m 00s, episodes 1628, frames 2.509M, mean epr -18.42\n",
      "time 07h 10m 00s, episodes 1645, frames 2.538M, mean epr -17.75\n",
      "time 07h 15m 00s, episodes 1661, frames 2.567M, mean epr -18.43\n",
      "time 07h 20m 00s, episodes 1679, frames 2.596M, mean epr -18.25\n",
      "time 07h 25m 00s, episodes 1694, frames 2.624M, mean epr -18.28\n",
      "time 07h 30m 00s, episodes 1711, frames 2.653M, mean epr -18.23\n",
      "time 07h 35m 00s, episodes 1726, frames 2.682M, mean epr -17.40\n",
      "time 07h 40m 00s, episodes 1743, frames 2.711M, mean epr -17.38\n",
      "time 07h 45m 00s, episodes 1759, frames 2.740M, mean epr -17.57\n",
      "time 07h 50m 00s, episodes 1776, frames 2.770M, mean epr -17.84\n",
      "time 07h 55m 00s, episodes 1793, frames 2.799M, mean epr -16.87\n",
      "time 08h 00m 00s, episodes 1809, frames 2.828M, mean epr -17.42\n",
      "time 08h 05m 00s, episodes 1825, frames 2.857M, mean epr -18.07\n",
      "time 08h 10m 00s, episodes 1841, frames 2.886M, mean epr -18.11\n",
      "time 08h 15m 00s, episodes 1856, frames 2.915M, mean epr -17.17\n",
      "time 08h 20m 00s, episodes 1872, frames 2.944M, mean epr -17.38\n",
      "time 08h 25m 00s, episodes 1888, frames 2.974M, mean epr -17.74\n",
      "\n",
      "\t3M frames: saved model\n",
      "\n",
      "time 08h 30m 00s, episodes 1904, frames 3.003M, mean epr -17.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-553cc85efd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#processes = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1431d0e5d174>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, args, info, rank)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mstate_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mstate_sal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-402f692220fe>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# discounted rewards one liner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprepro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m195\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgazenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #if sys.version_info[0] > 2:\n",
    "    #    mp.set_start_method('spawn') # this must not be in global scope\n",
    "    #elif sys.platform == 'linux' or sys.platform == 'linux2':\n",
    "    #    raise \"Must be using Python 3 with linux!\" # or else you get a deadlock in conv2d\n",
    "    \n",
    "    args = get_args()\n",
    "    args.save_dir = '{}/'.format(args.env.lower()) # keep the directory structure simple\n",
    "    if args.render:  args.processes = 1 ; args.test = True # render mode -> test mode w one process\n",
    "    if args.test:  args.lr = 0 # don't train in render mode\n",
    "    args.num_actions = gym.make(args.env).action_space.n # get the action space of this game\n",
    "    os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc.\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions)\n",
    "    model.try_load(args.save_dir)\n",
    "    #shared_optimizer = SharedAdam(shared_model.parameters(), lr=args.lr)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr= args.lr)\n",
    "    \n",
    "    \n",
    "    info = {k: torch.DoubleTensor([0]) for k in ['run_epr', 'run_loss', 'episodes', 'frames']}\n",
    "    #info['frames'] += shared_model.try_load(args.save_dir) * 1e6\n",
    "    if int(info['frames'].item()) == 0: printlog(args,'', end='', mode='w') # clear log file\n",
    "      \n",
    "      \n",
    "    print('Model information:')\n",
    "    count = 0\n",
    "    for f in model.parameters():\n",
    "      count += np.prod(f.size())\n",
    "      print(f.size(), np.prod(f.size()))\n",
    "  \n",
    "    print(count)\n",
    "    print('-'*50)\n",
    "    \n",
    "    train(model,optimizer, args, info)\n",
    "    \n",
    "    #processes = []\n",
    "    #for rank in range(args.processes):\n",
    "    #    p = mp.Process(target=train, args=(shared_model, shared_optimizer, rank, args, info))\n",
    "    #    p.start() ; processes.append(p)\n",
    "    #for p in processes: p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 80])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = collections.deque(maxlen = 4)\n",
    "for _ in range(4):\n",
    "  seq.append(np.zeros((80,80)))\n",
    "\n",
    "  \n",
    "seq_0 = seq.copy()\n",
    "torch.tensor(seq).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: pong-v4/model.3.tar\n",
      "19.0\n",
      "18.0\n",
      "19.0\n",
      "19.0\n",
      "20.0\n",
      "18.0\n",
      "19.0\n",
      "14.0\n",
      "18.0\n",
      "16.0\n",
      "19.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "16.0\n",
      "17.0\n",
      "12.0\n",
      "20.0\n",
      "14.0\n",
      "15.0\n",
      "17.0\n",
      "13.0\n",
      "18.0\n",
      "21.0\n",
      "20.0\n",
      "18.0\n",
      "16.0\n",
      "18.0\n",
      "18.0\n",
      "15.0\n",
      "13.0\n",
      "19.0\n",
      "20.0\n",
      "15.0\n",
      "19.0\n",
      "19.0\n",
      "16.0\n",
      "17.0\n",
      "17.0\n",
      "17.0\n",
      "17.0\n",
      "17.0\n",
      "20.0\n",
      "18.0\n",
      "15.0\n",
      "19.0\n",
      "18.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "state = env.reset()\n",
    "\n",
    "args = get_args()\n",
    "args.save_dir = '{}/'.format(args.env.lower()) # keep the directory structure simple\n",
    "args.num_actions = gym.make(args.env).action_space.n # get the action space of this game\n",
    "\n",
    "seq.append(prepro(state))\n",
    "R = 0\n",
    "hx = torch.zeros(1,args.hidden)\n",
    "cx = torch.zeros(1,args.hidden)\n",
    "  \n",
    "  \n",
    "rewards = []\n",
    "render = True\n",
    "  \n",
    "model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions)\n",
    "model.try_load(args.save_dir)\n",
    " \n",
    "for i in range(50):\n",
    "  while True:\n",
    "    state_sal = get_saliency(seq)\n",
    "    state_val, action_val, hx, cx = model.forward((state_sal,hx,cx))\n",
    "  \n",
    "  \n",
    "    action_prob = F.softmax(action_val, dim = -1)\n",
    "    a = action_prob.multinomial(num_samples = 1).data[0]\n",
    "  \n",
    "    state, reward, done, _ = env.step(a)\n",
    "    seq.append(prepro(state))  \n",
    "      \n",
    "    R += reward\n",
    "    if done:\n",
    "      seq = seq_0.copy()\n",
    "      state = env.reset()\n",
    "      seq.append(prepro(state))\n",
    "      print(-R)\n",
    "      rewards.append(-R)\n",
    "      R = 0\n",
    "      break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rewards), len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_new_weights(new_weights, policy):\n",
    "  count = 0\n",
    "  for f in policy.parameters():\n",
    "    size = f.size()\n",
    "    new_w = new_weights[count:count+np.prod(size)]\n",
    "    f.data = torch.tensor(new_w.reshape(size)).float()\n",
    "    count += np.prod(size)\n",
    "\n",
    "def run_episode(weights,policy, env, render = False):\n",
    "  state = env.reset()\n",
    "  R = 0\n",
    "  hx = torch.zeros(1,memsize)\n",
    "  \n",
    "  initialize_new_weights(weights,policy)\n",
    "  \n",
    "  while True:\n",
    "    if render:\n",
    "      env.render()\n",
    "    state = torch.tensor(prepro(state)).view(1,1,80,80)\n",
    "    \n",
    "    state_val, action_val, hx = policy.forward((state,hx))\n",
    "  \n",
    "    action_prob = F.softmax(action_val, dim = -1)\n",
    "    a = action_prob.multinomial(num_samples = 1).data[0]\n",
    "  \n",
    "    state, reward, done, _ = env.step(a)\n",
    "      \n",
    "    R += reward\n",
    "    if done:\n",
    "      return -R\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "memsize = 8\n",
    "channels = 1\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "policy = NNPolicy(num_actions, channels, memsize)\n",
    "\n",
    "tot_weights = sum(np.prod(f.size()) for f in policy.parameters())\n",
    "\n",
    "initial_weights = np.random.rand(tot_weights)\n",
    "\n",
    "res = cma.fmin(run_episode, initial_weights, 1, {'maxfevals': 3000, 'ftarget':-10,}, args=([policy, env]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_episode(res[0],policy,env,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_weights = 0\n",
    "for f in policy.parameters():\n",
    "  size = f.size()\n",
    "  tot_weights += np.prod(size)\n",
    "  print(*tuple(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_weights = sum(np.prod(f.size()) for f in policy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_weights = np.random.rand(tot_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_new_weights(new_weights, policy):\n",
    "  count = 0\n",
    "  for f in policy.parameters():\n",
    "    size = f.size()\n",
    "    new_w = new_weights[count:count+np.prod(size)]\n",
    "    f.data = torch.tensor(new_w.reshape(size))\n",
    "    count += np.prod(size)\n",
    "    \n",
    "new_weights = np.ones(tot_weights)\n",
    "\n",
    "initialize_new_weights(new_weights,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, channels, memsize, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.gru = nn.GRUCell(32 * 5 * 5, memsize)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1), nn.Linear(memsize, num_actions)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inputs, hx = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        hx = self.gru(x.view(-1, 32 * 5 * 5), (hx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, channels = 1, memsize =32 , num_actions = 6):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1, bias = False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1, bias = False)\n",
    "        #self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1, bias = False)\n",
    "        #self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1, bias = False)\n",
    "        self.gru = nn.GRUCell(64 * 20 * 20, memsize , bias = False)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1, bias = False), nn.Linear(memsize, num_actions, bias = False)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inputs, hx = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        #x = F.elu(self.conv3(x))\n",
    "        #x = F.elu(self.conv4(x))\n",
    "        hx = self.gru(x.view(-1, 64 * 20 * 20), (hx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1= NNPolicy()\n",
    "print('Model information:')\n",
    "count = 0\n",
    "for f in m1.parameters():\n",
    "  count += np.prod(f.size())\n",
    "  print(f.size(), np.prod(f.size()))\n",
    "  \n",
    "print(count)\n",
    "\n",
    "rand_state = torch.tensor(np.random.rand(80,80)).float().view(1,1,80,80)\n",
    "hidden_state = torch.tensor(np.random.rand(1,32)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1((rand_state, hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
