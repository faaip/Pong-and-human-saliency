{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch, os, gym, time, glob, argparse, sys\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter\n",
    "from scipy.misc import imresize # preserves single-pixel info _unlike_ img = img[::2,::2]\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import collections\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Saliency Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GazeNetwork(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GazeNetwork, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(4, 32, 4, stride=2, bias = True)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, stride=2, bias = True)\n",
    "    self.conv3 = nn.Conv2d(64, 64, 5, stride=2, bias = True)\n",
    "    self.deconv3 = nn.ConvTranspose2d(64,64,5,stride = 2, bias = True)\n",
    "    self.deconv2 = nn.ConvTranspose2d(64,32,3,stride = 2, bias = True)\n",
    "    self.deconv1 = nn.ConvTranspose2d(32,1,4,stride = 2, bias = True)\n",
    "    self.logsoftmax = nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "  def forward(self, X, train=True, hard=False):\n",
    "    X = F.elu(self.conv1(X))\n",
    "    X = F.elu(self.conv2(X))\n",
    "    X = F.elu(self.conv3(X))\n",
    "    X = F.elu(self.deconv3(X))\n",
    "    X = F.elu(self.deconv2(X))\n",
    "    X = self.deconv1(X)\n",
    "    \n",
    "    return self.logsoftmax(X.view(-1,1,6400)).view(-1,1,80,80)\n",
    "  \n",
    "  def load_model(self, path):\n",
    "    self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gazenet = GazeNetwork()#.train(False)\n",
    "gazenet.load_model('model/saliency_model_5.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Make  the policy trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--env', default='Pong-v4', type=str, help='gym environment')\n",
    "    parser.add_argument('--processes', default=20, type=int, help='number of processes to train with')\n",
    "    parser.add_argument('--render', default=False, type=bool, help='renders the atari environment')\n",
    "    parser.add_argument('--test', default=False, type=bool, help='sets lr=0, chooses most likely actions')\n",
    "    parser.add_argument('--rnn_steps', default=50, type=int, help='steps to train LSTM over')\n",
    "    parser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
    "    parser.add_argument('--seed', default=1, type=int, help='seed random # generators (for reproducibility)')\n",
    "    parser.add_argument('--gamma', default=0.99, type=float, help='rewards discount factor')\n",
    "    parser.add_argument('--tau', default=1.0, type=float, help='generalized advantage estimation discount')\n",
    "    parser.add_argument('--horizon', default=0.8, type=float, help='horizon for running averages')\n",
    "    parser.add_argument('--hidden', default=32, type=int, help='hidden size of GRU')\n",
    "    return parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discount = lambda x, gamma: lfilter([1],[1,-gamma],x[::-1])[::-1] # discounted rewards one liner\n",
    "prepro = lambda img: imresize(img[35:195].mean(2), (80,80)).astype(np.float32)/255.\n",
    "\n",
    "def get_saliency(states, net= gazenet):\n",
    "  sal = net(torch.FloatTensor(states).view(1,4,80,80))\n",
    "  sal = sal.data.numpy()\n",
    "  sal = np.exp(sal)\n",
    "  sal = sal/sal.max()\n",
    "  sal = np.minimum(sal,.1)\n",
    "  return torch.tensor(np.multiply(sal, states[-1]))\n",
    "  \n",
    "def printlog(args, s, end='\\n', mode='a'):\n",
    "    print(s, end=end) ; f=open(args.save_dir+'log.txt',mode) ; f.write(s+'\\n') ; f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v4')\n",
    "state = env.reset()\n",
    "frames = []\n",
    "seq = collections.deque(maxlen = 4)\n",
    "for _ in range(4):\n",
    "  seq.append(np.zeros((80,80)))\n",
    "\n",
    "state = prepro(state)\n",
    "seq.append(state)\n",
    "for i in range(50):\n",
    "  action = np.random.randint(6)\n",
    "  frame, reward, done, _ = env.step(action)\n",
    "  \n",
    "  state = prepro(frame)\n",
    "  seq.append(state)\n",
    "  state_sal = get_saliency(seq)\n",
    "  frames.append(state_sal)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 80, 80])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJsAAAJCCAYAAADXz7VCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109b85390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "fig, ax = plt.subplots(nrows = 4, figsize = (20,10))\n",
    "\n",
    "print(frames[10].data.numpy()[0,0,10,:])\n",
    "for i in range(1,5):\n",
    "  ax[i-1].imshow(frames[i*10].data.numpy()[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.004187107086182"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = []\n",
    "for _ in range(50):\n",
    "  start = time.time()\n",
    "\n",
    "  k = get_saliency(seq)\n",
    "\n",
    "  times.append(time.time()-start)\n",
    "26000*np.mean(times)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, num_actions, channels, memsize):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 4, 3, stride=2, padding=1, bias = None)\n",
    "        self.conv2 = nn.Conv2d(4, 8, 3, stride=2, padding=1, bias = None)\n",
    "        self.conv3 = nn.Conv2d(8, 16, 5, stride=4, padding=2, bias = None)\n",
    "        self.conv4 = nn.Conv2d(16, 32, 3, stride=2, padding=1, bias = None)\n",
    "        self.conv5 = nn.Conv2d(32, 64, 3, stride=1, padding=0, bias = None)\n",
    "        self.mem = nn.LSTMCell(64, memsize, bias = None)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1, bias = None), nn.Linear(memsize, num_actions, bias = None)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inp, hx, cx = inputs\n",
    "        x = F.elu(self.conv1(inp))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = F.elu(self.conv5(x))\n",
    "        hx, cx = self.mem(x.view(-1, 64 * 1 * 1), (hx, cx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx, cx\n",
    "\n",
    "    def try_load(self, save_dir):\n",
    "        paths = glob.glob(save_dir + '*.tar') ; step = 0\n",
    "        if len(paths) > 0:\n",
    "            ckpts = [int(s.split('.')[-2]) for s in paths]\n",
    "            ix = np.argmax(ckpts) ; step = ckpts[ix]\n",
    "            self.load_state_dict(torch.load(paths[ix]))\n",
    "        print(\"\\tno saved models\") if step is 0 else print(\"\\tloaded model: {}\".format(paths[ix]))\n",
    "        return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 3]) 36\n",
      "torch.Size([8, 4, 3, 3]) 288\n",
      "torch.Size([16, 8, 5, 5]) 3200\n",
      "torch.Size([32, 16, 3, 3]) 4608\n",
      "torch.Size([64, 32, 3, 3]) 18432\n",
      "torch.Size([128, 64]) 8192\n",
      "torch.Size([128, 32]) 4096\n",
      "torch.Size([1, 32]) 32\n",
      "torch.Size([6, 32]) 192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39076"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NNPolicy(6,1,32)\n",
    "count = 0\n",
    "for f in model.parameters():\n",
    "  count += np.prod(f.size())\n",
    "  print(f.size(), np.prod(f.size()))\n",
    "  \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4608"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*16*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_func(args, values, logps, actions, rewards):\n",
    "    np_values = values.view(-1).data.numpy()\n",
    "\n",
    "    # generalized advantage estimation using \\delta_t residuals (a policy gradient method)\n",
    "    delta_t = np.asarray(rewards) + args.gamma * np_values[1:] - np_values[:-1]\n",
    "    logpys = logps.gather(1, torch.tensor(actions).view(-1,1))\n",
    "    gen_adv_est = discount(delta_t, args.gamma * args.tau)\n",
    "    policy_loss = -(logpys.view(-1) * torch.FloatTensor(gen_adv_est.copy())).sum()\n",
    "    \n",
    "    # l2 loss over value estimator\n",
    "    rewards[-1] += args.gamma * np_values[-1]\n",
    "    discounted_r = discount(np.asarray(rewards), args.gamma)\n",
    "    discounted_r = torch.tensor(discounted_r.copy(), dtype=torch.float32)\n",
    "    value_loss = .5 * (discounted_r - values[:-1,0]).pow(2).sum()\n",
    "    \n",
    "    entropy_loss = -(-logps * torch.exp(logps)).sum() # encourage lower entropy\n",
    "    \n",
    "    return policy_loss + 0.5 * value_loss# + 0.01 * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, args, info, rank = 0):\n",
    "    env = gym.make(args.env) # make a local (unshared) environment\n",
    "    \n",
    "    # seed everything\n",
    "    #env.seed(args.seed + rank)\n",
    "    #torch.manual_seed(args.seed + rank) \n",
    "    \n",
    "    #model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions) # a local/unshared model\n",
    "    \n",
    "    state_seq = collections.deque(maxlen = 4)\n",
    "    for _ in range(4):\n",
    "      state_seq.append(np.zeros((80,80)))\n",
    "    \n",
    "    state = prepro(env.reset()) # get first state\n",
    "    state_seq.append(state)\n",
    "    state_sal = get_saliency(state_seq)\n",
    "\n",
    "    start_time = last_disp_time = time.time()\n",
    "    episode_length, epr, eploss, done  = 0, 0, 0, True # bookkeeping\n",
    "\n",
    "    while info['frames'][0] <= 8e7 or args.test: # openai baselines uses 40M frames...we'll use 80M\n",
    "        #model.load_state_dict(shared_model.state_dict()) # sync with shared model\n",
    "        \n",
    "        hx = torch.zeros(1, args.hidden) if done else hx.detach()  # rnn activation vector\n",
    "        cx = torch.zeros(1, args.hidden) if done else cx.detach()\n",
    "          \n",
    "        values, logps, actions, rewards = [], [], [], [] # save values for computing gradientss\n",
    "\n",
    "        for step in range(args.rnn_steps):\n",
    "            episode_length += 1\n",
    "            \n",
    "            value, logit, hx, cx = model((state_sal.view(1,1,80,80), hx,cx))\n",
    "            logp = F.log_softmax(logit, dim=-1)                             \n",
    "            \n",
    "            action = torch.exp(logp).multinomial(num_samples=1).data[0]#logp.max(1)[1].data if args.test else\n",
    "                                         \n",
    "            state, reward, done, _ = env.step(action.numpy()[0])\n",
    "            \n",
    "            if args.render: env.render()\n",
    "\n",
    "              \n",
    "            start = time.time()\n",
    "            state_seq.append(prepro(state))\n",
    "            state_sal = get_saliency(state_seq)\n",
    "            \n",
    "            epr += reward\n",
    "                                         \n",
    "            reward = np.clip(reward, -1, 1) # reward\n",
    "            done = done or episode_length >= 1e4 # don't playing one ep for too long\n",
    "            \n",
    "            info['frames'].add_(1)\n",
    "            num_frames = int(info['frames'].item())\n",
    "                                         \n",
    "            if num_frames % 1e6 == 0: # save every 1M frames\n",
    "                printlog(args, '\\n\\t{:.0f}M frames: saved model\\n'.format(num_frames/1e6))\n",
    "                torch.save(model.state_dict(), args.save_dir+'model.{:.0f}.tar'.format(num_frames/1e6))\n",
    "\n",
    "            if done: # update shared data\n",
    "                info['episodes'] += 1\n",
    "                interp = 1 if info['episodes'][0] == 1 else 1 - args.horizon\n",
    "                info['run_epr'].mul_(1-interp).add_(interp * epr)\n",
    "\n",
    "            if time.time() - last_disp_time > 300: # print info ~ every 5 minute\n",
    "                elapsed = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start_time))\n",
    "                printlog(args, 'time {}, episodes {:.0f}, frames {:.3f}M, mean epr {:.2f}'\n",
    "                    .format(elapsed, info['episodes'].item(), num_frames/1e6,\n",
    "                    info['run_epr'].item()))\n",
    "                last_disp_time = time.time()\n",
    "\n",
    "            if done: # maybe print info.\n",
    "                episode_length, epr = 0, 0\n",
    "                state = torch.tensor(prepro(env.reset()))\n",
    "\n",
    "            values.append(value) ; logps.append(logp) ; actions.append(action) ; rewards.append(reward)\n",
    "        \n",
    "        # add a final state value function for calculating the losses.\n",
    "        next_value = torch.zeros(1,1) if done else model((state_sal.view(1,1,80,80), hx,cx))[0]\n",
    "        values.append(next_value.detach())\n",
    "\n",
    "        loss = cost_func(args, torch.cat(values), torch.cat(logps), torch.cat(actions), np.asarray(rewards))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "        #\n",
    "        #for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "        #    if shared_param.grad is None: shared_param._grad = param.grad # sync gradients with shared model\n",
    "        #shared_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: pong-v4/model.3.tar\n",
      "Model information:\n",
      "torch.Size([4, 1, 3, 3]) 36\n",
      "torch.Size([8, 4, 3, 3]) 288\n",
      "torch.Size([16, 8, 5, 5]) 3200\n",
      "torch.Size([32, 16, 3, 3]) 4608\n",
      "torch.Size([64, 32, 3, 3]) 18432\n",
      "torch.Size([128, 64]) 8192\n",
      "torch.Size([128, 32]) 4096\n",
      "torch.Size([1, 32]) 32\n",
      "torch.Size([6, 32]) 192\n",
      "39076\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 00h 05m 00s, episodes 19, frames 0.028M, mean epr -19.00\n",
      "time 00h 10m 00s, episodes 40, frames 0.058M, mean epr -19.47\n",
      "time 00h 15m 00s, episodes 62, frames 0.089M, mean epr -19.66\n",
      "time 00h 20m 00s, episodes 82, frames 0.120M, mean epr -18.96\n",
      "time 00h 25m 00s, episodes 103, frames 0.150M, mean epr -19.88\n",
      "time 00h 30m 00s, episodes 124, frames 0.181M, mean epr -18.77\n",
      "time 00h 35m 00s, episodes 144, frames 0.210M, mean epr -19.14\n",
      "time 00h 40m 00s, episodes 165, frames 0.241M, mean epr -19.20\n",
      "time 00h 45m 00s, episodes 187, frames 0.270M, mean epr -19.73\n",
      "time 00h 50m 00s, episodes 209, frames 0.301M, mean epr -19.59\n",
      "time 00h 55m 00s, episodes 229, frames 0.331M, mean epr -17.77\n",
      "time 01h 00m 00s, episodes 249, frames 0.359M, mean epr -19.48\n",
      "time 01h 05m 00s, episodes 269, frames 0.388M, mean epr -18.50\n",
      "time 01h 10m 00s, episodes 288, frames 0.416M, mean epr -19.16\n",
      "time 01h 15m 00s, episodes 306, frames 0.443M, mean epr -19.60\n",
      "time 01h 20m 00s, episodes 327, frames 0.472M, mean epr -19.43\n",
      "time 01h 25m 00s, episodes 347, frames 0.502M, mean epr -19.69\n",
      "time 01h 30m 00s, episodes 367, frames 0.531M, mean epr -19.60\n",
      "time 01h 35m 00s, episodes 387, frames 0.559M, mean epr -19.37\n",
      "time 01h 40m 00s, episodes 407, frames 0.589M, mean epr -18.97\n",
      "time 01h 45m 00s, episodes 427, frames 0.619M, mean epr -18.75\n",
      "time 01h 50m 00s, episodes 447, frames 0.648M, mean epr -19.45\n",
      "time 01h 55m 00s, episodes 466, frames 0.677M, mean epr -19.15\n",
      "time 02h 00m 00s, episodes 488, frames 0.708M, mean epr -19.77\n",
      "time 02h 05m 00s, episodes 507, frames 0.738M, mean epr -18.80\n",
      "time 02h 10m 00s, episodes 526, frames 0.767M, mean epr -18.83\n",
      "time 02h 15m 00s, episodes 547, frames 0.798M, mean epr -19.14\n",
      "time 02h 20m 00s, episodes 568, frames 0.827M, mean epr -19.48\n",
      "time 02h 25m 00s, episodes 585, frames 0.854M, mean epr -18.03\n",
      "time 02h 30m 00s, episodes 605, frames 0.883M, mean epr -19.57\n",
      "time 02h 35m 00s, episodes 624, frames 0.912M, mean epr -18.98\n",
      "time 02h 40m 00s, episodes 644, frames 0.942M, mean epr -18.51\n",
      "time 02h 45m 00s, episodes 663, frames 0.972M, mean epr -18.81\n",
      "\n",
      "\t1M frames: saved model\n",
      "\n",
      "time 02h 50m 00s, episodes 682, frames 1.001M, mean epr -18.96\n",
      "time 02h 55m 00s, episodes 702, frames 1.029M, mean epr -18.93\n",
      "time 03h 00m 00s, episodes 722, frames 1.059M, mean epr -19.19\n",
      "time 03h 05m 00s, episodes 739, frames 1.084M, mean epr -19.61\n",
      "time 03h 10m 00s, episodes 758, frames 1.113M, mean epr -18.42\n",
      "time 03h 15m 00s, episodes 777, frames 1.142M, mean epr -19.46\n",
      "time 03h 20m 00s, episodes 796, frames 1.171M, mean epr -18.68\n",
      "time 03h 25m 00s, episodes 815, frames 1.199M, mean epr -18.95\n",
      "time 03h 30m 00s, episodes 835, frames 1.228M, mean epr -18.48\n",
      "time 03h 35m 00s, episodes 854, frames 1.259M, mean epr -17.92\n",
      "time 03h 40m 00s, episodes 874, frames 1.289M, mean epr -19.07\n",
      "time 03h 45m 00s, episodes 893, frames 1.319M, mean epr -18.14\n",
      "time 03h 50m 00s, episodes 913, frames 1.349M, mean epr -18.92\n",
      "time 03h 55m 00s, episodes 933, frames 1.380M, mean epr -18.95\n",
      "time 04h 00m 00s, episodes 953, frames 1.410M, mean epr -18.76\n",
      "time 04h 05m 00s, episodes 972, frames 1.440M, mean epr -18.49\n",
      "time 04h 10m 00s, episodes 990, frames 1.467M, mean epr -19.41\n",
      "time 04h 15m 00s, episodes 1009, frames 1.495M, mean epr -18.82\n",
      "time 04h 20m 00s, episodes 1027, frames 1.524M, mean epr -19.04\n",
      "time 04h 25m 00s, episodes 1046, frames 1.553M, mean epr -18.65\n",
      "time 04h 30m 00s, episodes 1064, frames 1.583M, mean epr -18.01\n",
      "time 04h 35m 00s, episodes 1082, frames 1.612M, mean epr -18.19\n",
      "time 04h 40m 00s, episodes 1099, frames 1.641M, mean epr -18.09\n",
      "time 04h 45m 00s, episodes 1118, frames 1.672M, mean epr -18.78\n",
      "time 04h 50m 00s, episodes 1137, frames 1.702M, mean epr -19.15\n",
      "time 04h 55m 00s, episodes 1157, frames 1.732M, mean epr -18.94\n",
      "time 05h 00m 00s, episodes 1177, frames 1.762M, mean epr -20.04\n",
      "time 05h 05m 00s, episodes 1196, frames 1.792M, mean epr -18.59\n",
      "time 05h 10m 00s, episodes 1214, frames 1.822M, mean epr -18.60\n",
      "time 05h 15m 00s, episodes 1233, frames 1.853M, mean epr -18.72\n",
      "time 05h 20m 00s, episodes 1252, frames 1.883M, mean epr -18.70\n",
      "time 05h 25m 00s, episodes 1271, frames 1.913M, mean epr -19.09\n",
      "time 05h 30m 00s, episodes 1289, frames 1.944M, mean epr -18.83\n",
      "time 05h 35m 00s, episodes 1307, frames 1.974M, mean epr -18.19\n",
      "\n",
      "\t2M frames: saved model\n",
      "\n",
      "time 05h 40m 00s, episodes 1327, frames 2.004M, mean epr -19.06\n",
      "time 05h 45m 00s, episodes 1345, frames 2.035M, mean epr -17.79\n",
      "time 05h 50m 00s, episodes 1365, frames 2.065M, mean epr -19.22\n",
      "time 05h 55m 00s, episodes 1383, frames 2.095M, mean epr -17.19\n",
      "time 06h 00m 00s, episodes 1402, frames 2.126M, mean epr -18.57\n",
      "time 06h 05m 00s, episodes 1421, frames 2.156M, mean epr -18.78\n",
      "time 06h 10m 00s, episodes 1440, frames 2.186M, mean epr -18.91\n",
      "time 06h 15m 00s, episodes 1457, frames 2.216M, mean epr -18.34\n",
      "time 06h 20m 00s, episodes 1475, frames 2.246M, mean epr -18.32\n",
      "time 06h 25m 00s, episodes 1493, frames 2.276M, mean epr -18.64\n",
      "time 06h 30m 00s, episodes 1511, frames 2.306M, mean epr -17.17\n",
      "time 06h 35m 00s, episodes 1527, frames 2.335M, mean epr -18.29\n",
      "time 06h 40m 00s, episodes 1545, frames 2.364M, mean epr -18.41\n",
      "time 06h 45m 00s, episodes 1562, frames 2.393M, mean epr -17.49\n",
      "time 06h 50m 00s, episodes 1578, frames 2.422M, mean epr -18.19\n",
      "time 06h 55m 00s, episodes 1595, frames 2.451M, mean epr -17.86\n",
      "time 07h 00m 00s, episodes 1611, frames 2.479M, mean epr -17.75\n",
      "time 07h 05m 00s, episodes 1628, frames 2.509M, mean epr -18.42\n",
      "time 07h 10m 00s, episodes 1645, frames 2.538M, mean epr -17.75\n",
      "time 07h 15m 00s, episodes 1661, frames 2.567M, mean epr -18.43\n",
      "time 07h 20m 00s, episodes 1679, frames 2.596M, mean epr -18.25\n",
      "time 07h 25m 00s, episodes 1694, frames 2.624M, mean epr -18.28\n",
      "time 07h 30m 00s, episodes 1711, frames 2.653M, mean epr -18.23\n",
      "time 07h 35m 00s, episodes 1726, frames 2.682M, mean epr -17.40\n",
      "time 07h 40m 00s, episodes 1743, frames 2.711M, mean epr -17.38\n",
      "time 07h 45m 00s, episodes 1759, frames 2.740M, mean epr -17.57\n",
      "time 07h 50m 00s, episodes 1776, frames 2.770M, mean epr -17.84\n",
      "time 07h 55m 00s, episodes 1793, frames 2.799M, mean epr -16.87\n",
      "time 08h 00m 00s, episodes 1809, frames 2.828M, mean epr -17.42\n",
      "time 08h 05m 00s, episodes 1825, frames 2.857M, mean epr -18.07\n",
      "time 08h 10m 00s, episodes 1841, frames 2.886M, mean epr -18.11\n",
      "time 08h 15m 00s, episodes 1856, frames 2.915M, mean epr -17.17\n",
      "time 08h 20m 00s, episodes 1872, frames 2.944M, mean epr -17.38\n",
      "time 08h 25m 00s, episodes 1888, frames 2.974M, mean epr -17.74\n",
      "\n",
      "\t3M frames: saved model\n",
      "\n",
      "time 08h 30m 00s, episodes 1904, frames 3.003M, mean epr -17.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-553cc85efd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#processes = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1431d0e5d174>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, args, info, rank)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mstate_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mstate_sal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-402f692220fe>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# discounted rewards one liner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprepro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m195\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_saliency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgazenet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #if sys.version_info[0] > 2:\n",
    "    #    mp.set_start_method('spawn') # this must not be in global scope\n",
    "    #elif sys.platform == 'linux' or sys.platform == 'linux2':\n",
    "    #    raise \"Must be using Python 3 with linux!\" # or else you get a deadlock in conv2d\n",
    "    \n",
    "    args = get_args()\n",
    "    args.save_dir = '{}/'.format(args.env.lower()) # keep the directory structure simple\n",
    "    if args.render:  args.processes = 1 ; args.test = True # render mode -> test mode w one process\n",
    "    if args.test:  args.lr = 0 # don't train in render mode\n",
    "    args.num_actions = gym.make(args.env).action_space.n # get the action space of this game\n",
    "    os.makedirs(args.save_dir) if not os.path.exists(args.save_dir) else None # make dir to save models etc.\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions)\n",
    "    model.try_load(args.save_dir)\n",
    "    #shared_optimizer = SharedAdam(shared_model.parameters(), lr=args.lr)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr= args.lr)\n",
    "    \n",
    "    \n",
    "    info = {k: torch.DoubleTensor([0]) for k in ['run_epr', 'run_loss', 'episodes', 'frames']}\n",
    "    #info['frames'] += shared_model.try_load(args.save_dir) * 1e6\n",
    "    if int(info['frames'].item()) == 0: printlog(args,'', end='', mode='w') # clear log file\n",
    "      \n",
    "      \n",
    "    print('Model information:')\n",
    "    count = 0\n",
    "    for f in model.parameters():\n",
    "      count += np.prod(f.size())\n",
    "      print(f.size(), np.prod(f.size()))\n",
    "  \n",
    "    print(count)\n",
    "    print('-'*50)\n",
    "    \n",
    "    train(model,optimizer, args, info)\n",
    "    \n",
    "    #processes = []\n",
    "    #for rank in range(args.processes):\n",
    "    #    p = mp.Process(target=train, args=(shared_model, shared_optimizer, rank, args, info))\n",
    "    #    p.start() ; processes.append(p)\n",
    "    #for p in processes: p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 80])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = collections.deque(maxlen = 4)\n",
    "for _ in range(4):\n",
    "  seq.append(np.zeros((80,80)))\n",
    "\n",
    "torch.tensor(seq).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloaded model: pong-v4/model.3.tar\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "state = env.reset()\n",
    "\n",
    "seq.append(prepro(state))\n",
    "R = 0\n",
    "hx = torch.zeros(1,args.hidden)\n",
    "cx = torch.zeros(1,args.hidden)\n",
    "  \n",
    "render = True\n",
    "  \n",
    "model = NNPolicy(channels=1, memsize=args.hidden, num_actions=args.num_actions)\n",
    "model.try_load(args.save_dir)\n",
    "  \n",
    "while True:\n",
    "  if render:\n",
    "    env.render()\n",
    "    \n",
    "    state_sal = get_saliency(seq)\n",
    "    state_val, action_val, hx, cx = model.forward((state_sal,hx,cx))\n",
    "  \n",
    "  \n",
    "    action_prob = F.softmax(action_val, dim = -1)\n",
    "    a = action_prob.multinomial(num_samples = 1).data[0]\n",
    "  \n",
    "    state, reward, done, _ = env.step(a)\n",
    "    seq.append(prepro(state))  \n",
    "      \n",
    "    R += reward\n",
    "    if done:\n",
    "      print(-R)\n",
    "      break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_new_weights(new_weights, policy):\n",
    "  count = 0\n",
    "  for f in policy.parameters():\n",
    "    size = f.size()\n",
    "    new_w = new_weights[count:count+np.prod(size)]\n",
    "    f.data = torch.tensor(new_w.reshape(size)).float()\n",
    "    count += np.prod(size)\n",
    "\n",
    "def run_episode(weights,policy, env, render = False):\n",
    "  state = env.reset()\n",
    "  R = 0\n",
    "  hx = torch.zeros(1,memsize)\n",
    "  \n",
    "  initialize_new_weights(weights,policy)\n",
    "  \n",
    "  while True:\n",
    "    if render:\n",
    "      env.render()\n",
    "    state = torch.tensor(prepro(state)).view(1,1,80,80)\n",
    "    \n",
    "    state_val, action_val, hx = policy.forward((state,hx))\n",
    "  \n",
    "    action_prob = F.softmax(action_val, dim = -1)\n",
    "    a = action_prob.multinomial(num_samples = 1).data[0]\n",
    "  \n",
    "    state, reward, done, _ = env.step(a)\n",
    "      \n",
    "    R += reward\n",
    "    if done:\n",
    "      return -R\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "memsize = 8\n",
    "channels = 1\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "policy = NNPolicy(num_actions, channels, memsize)\n",
    "\n",
    "tot_weights = sum(np.prod(f.size()) for f in policy.parameters())\n",
    "\n",
    "initial_weights = np.random.rand(tot_weights)\n",
    "\n",
    "res = cma.fmin(run_episode, initial_weights, 1, {'maxfevals': 3000, 'ftarget':-10,}, args=([policy, env]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_episode(res[0],policy,env,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_weights = 0\n",
    "for f in policy.parameters():\n",
    "  size = f.size()\n",
    "  tot_weights += np.prod(size)\n",
    "  print(*tuple(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_weights = sum(np.prod(f.size()) for f in policy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_weights = np.random.rand(tot_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_new_weights(new_weights, policy):\n",
    "  count = 0\n",
    "  for f in policy.parameters():\n",
    "    size = f.size()\n",
    "    new_w = new_weights[count:count+np.prod(size)]\n",
    "    f.data = torch.tensor(new_w.reshape(size))\n",
    "    count += np.prod(size)\n",
    "    \n",
    "new_weights = np.ones(tot_weights)\n",
    "\n",
    "initialize_new_weights(new_weights,policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, channels, memsize, num_actions):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.gru = nn.GRUCell(32 * 5 * 5, memsize)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1), nn.Linear(memsize, num_actions)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inputs, hx = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        hx = self.gru(x.view(-1, 32 * 5 * 5), (hx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNPolicy(nn.Module): # an actor-critic neural network\n",
    "    def __init__(self, channels = 1, memsize =32 , num_actions = 6):\n",
    "        super(NNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 3, stride=2, padding=1, bias = False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1, bias = False)\n",
    "        #self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1, bias = False)\n",
    "        #self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1, bias = False)\n",
    "        self.gru = nn.GRUCell(64 * 20 * 20, memsize , bias = False)\n",
    "        self.critic_linear, self.actor_linear = nn.Linear(memsize, 1, bias = False), nn.Linear(memsize, num_actions, bias = False)\n",
    "\n",
    "    def forward(self, inputs, train=True, hard=False):\n",
    "        inputs, hx = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        #x = F.elu(self.conv3(x))\n",
    "        #x = F.elu(self.conv4(x))\n",
    "        hx = self.gru(x.view(-1, 64 * 20 * 20), (hx))\n",
    "        return self.critic_linear(hx), self.actor_linear(hx), hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1= NNPolicy()\n",
    "print('Model information:')\n",
    "count = 0\n",
    "for f in m1.parameters():\n",
    "  count += np.prod(f.size())\n",
    "  print(f.size(), np.prod(f.size()))\n",
    "  \n",
    "print(count)\n",
    "\n",
    "rand_state = torch.tensor(np.random.rand(80,80)).float().view(1,1,80,80)\n",
    "hidden_state = torch.tensor(np.random.rand(1,32)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1((rand_state, hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
